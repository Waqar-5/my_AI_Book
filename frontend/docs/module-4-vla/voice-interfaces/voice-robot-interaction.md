# Voice-Robot Interaction Design

## Introduction

Effective voice-robot interaction is crucial for creating intuitive and user-friendly humanoid robots. This chapter explores how to design and implement interaction patterns that allow users to communicate naturally with robots using voice commands. We'll cover the complete interaction cycle from receiving voice input to providing appropriate feedback to the user.

## Principles of Voice-Robot Interaction

### Natural Language Understanding

Designing voice interfaces for robots requires understanding how humans naturally communicate:

1. **Conversational Flow**: Humans expect back-and-forth communication
2. **Context Awareness**: Humans expect robots to remember context from previous interactions
3. **Feedback Expectations**: Humans expect acknowledgment of their commands
4. **Error Recovery**: Humans expect graceful handling of misunderstood commands

### User Experience Considerations

When designing voice interactions, consider the user experience:

- **Response Time**: Keep response times under 2-3 seconds for natural-feeling interactions
- **Clarity**: Provide clear feedback about robot's understanding and status
- **Flexibility**: Accept various ways of expressing the same request
- **Robustness**: Gracefully handle ambiguous or unclear commands

## Interaction Patterns

### Direct Command Pattern

The simplest interaction pattern where users give direct commands:

```
User: "Move forward 2 meters"
Robot: [Moves forward 2 meters] "I've moved forward 2 meters."
```

Implementation of direct command pattern:

```python
class DirectCommandHandler:
    def __init__(self, intent_mapper, action_mapper):
        self.intent_mapper = intent_mapper
        self.action_mapper = action_mapper
        self.speech_synthesizer = None  # Text-to-speech system
    
    def handle_direct_command(self, voice_command):
        """
        Handle a direct command from the user
        """
        # Classify intent from voice command
        intent_result = self.intent_mapper.classify_with_entities(voice_command)
        
        if intent_result["intent"] == "unknown":
            return self.handle_unknown_command(voice_command)
        
        # Map intent to robot actions
        action_result = self.action_mapper.map_intent_to_action(
            intent_result, intent_result["entities"]
        )
        
        if "error" in action_result:
            return self.handle_action_error(action_result["error"])
        
        # Execute the action sequence
        execution_result = self.execute_action_sequence(action_result["action_sequence"])
        
        # Provide feedback to the user
        feedback = self.generate_feedback(intent_result, execution_result)
        self.provide_feedback(feedback)
        
        return {
            "status": "success",
            "intent": intent_result["intent"],
            "action_sequence": action_result["action_sequence"],
            "feedback": feedback
        }
    
    def handle_unknown_command(self, command):
        """
        Handle commands that couldn't be classified
        """
        feedback = f"I didn't understand your command: {command}. Could you please rephrase?"
        self.provide_feedback(feedback)
        
        return {
            "status": "unknown_command",
            "command": command,
            "feedback": feedback
        }
    
    def handle_action_error(self, error):
        """
        Handle errors in action execution
        """
        feedback = f"I encountered an error: {error}. Can you try a different command?"
        self.provide_feedback(feedback)
        
        return {
            "status": "action_error",
            "error": error,
            "feedback": feedback
        }
    
    def execute_action_sequence(self, actions):
        """
        Execute a sequence of robot actions
        """
        results = []
        for action in actions:
            try:
                # Execute the action on the robot
                result = self.execute_single_action(action)
                results.append({
                    "action": action,
                    "status": "success",
                    "result": result
                })
            except Exception as e:
                results.append({
                    "action": action,
                    "status": "failed",
                    "error": str(e)
                })
        
        return results
    
    def generate_feedback(self, intent_result, execution_result):
        """
        Generate feedback based on intent and execution results
        """
        intent = intent_result["intent"]
        
        # Check if all actions succeeded
        all_successful = all(action["status"] == "success" for action in execution_result)
        
        if not all_successful:
            failed_actions = [a for a in execution_result if a["status"] == "failed"]
            return f"I couldn't complete the action {intent}. Encountered errors: {[f['error'] for f in failed_actions]}"
        
        # Generate positive feedback based on intent
        if intent == "move_forward":
            return "I've moved forward as requested."
        elif intent == "turn_right":
            return "I've turned right as requested."
        elif intent == "pick_up":
            return "I've picked up the object as requested."
        elif intent == "navigate_to":
            location = intent_result["entities"].get("locations", ["location"])[0]
            return f"I've navigated to the {location} as requested."
        else:
            return f"I've completed the action {intent} as requested."
    
    def provide_feedback(self, message):
        """
        Provide feedback to the user (e.g., via speech synthesis)
        """
        if self.speech_synthesizer:
            self.speech_synthesizer.speak(message)
        else:
            print(f"Robot says: {message}")
```

### Contextual Conversation Pattern

A more advanced pattern that maintains context across multiple exchanges:

```python
class ContextualConversationHandler:
    def __init__(self, intent_mapper, action_mapper):
        self.intent_mapper = intent_mapper
        self.action_mapper = action_mapper
        self.speech_synthesizer = None
        self.conversation_context = {
            "last_intent": None,
            "target_object": None,
            "current_location": None,
            "active_task": None
        }
    
    def handle_conversation_turn(self, voice_command):
        """
        Handle a turn in a contextual conversation
        """
        # Classify intent from voice command
        intent_result = self.intent_mapper.classify_with_entities(voice_command)
        
        # Update context based on the command and its entities
        self.update_context(intent_result)
        
        if intent_result["intent"] == "unknown":
            return self.handle_unknown_command(voice_command)
        
        # Check if the command refers to previous context
        resolved_command = self.resolve_context_references(intent_result, voice_command)
        
        # Map intent to robot actions
        action_result = self.action_mapper.map_intent_to_action(
            resolved_command, resolved_command["entities"]
        )
        
        if "error" in action_result:
            return self.handle_action_error(action_result["error"])
        
        # Execute the action sequence
        execution_result = self.execute_action_sequence(action_result["action_sequence"])
        
        # Update context based on execution results
        self.update_context_post_execution(action_result, execution_result)
        
        # Provide feedback to the user
        feedback = self.generate_contextual_feedback(intent_result, execution_result)
        self.provide_feedback(feedback)
        
        return {
            "status": "success",
            "intent": intent_result["intent"],
            "action_sequence": action_result["action_sequence"],
            "feedback": feedback,
            "context": self.conversation_context.copy()
        }
    
    def update_context(self, intent_result):
        """
        Update conversation context based on the intent result
        """
        entities = intent_result.get("entities", {})
        
        if "objects" in entities and entities["objects"]:
            self.conversation_context["target_object"] = entities["objects"][0]
        
        if "locations" in entities and entities["locations"]:
            self.conversation_context["current_location"] = entities["locations"][0]
        
        if intent_result["intent"] != "unknown":
            self.conversation_context["last_intent"] = intent_result["intent"]
    
    def resolve_context_references(self, intent_result, original_command):
        """
        Resolve references to previous context (e.g., "it", "there", "same object")
        """
        resolved_result = intent_result.copy()
        entities = resolved_result.get("entities", {}).copy()
        
        # Replace pronouns with previous context
        if "it" in original_command.lower():
            if self.conversation_context["target_object"]:
                # Replace "it" with the previously referenced object
                entities["objects"] = [self.conversation_context["target_object"]]
        
        # Update the resolved result
        resolved_result["entities"] = entities
        return resolved_result
    
    def update_context_post_execution(self, action_result, execution_result):
        """
        Update context after actions have been executed
        """
        intent = action_result.get("intent")
        
        # Update context based on action completion
        if intent == "pick_up" and execution_result and execution_result[0]["status"] == "success":
            self.conversation_context["active_task"] = "holding_object"
        elif intent == "put_down":
            self.conversation_context["active_task"] = None
    
    def generate_contextual_feedback(self, intent_result, execution_result):
        """
        Generate feedback that acknowledges the conversation context
        """
        intent = intent_result["intent"]
        context = self.conversation_context
        
        # Check if all actions succeeded
        all_successful = all(action["status"] == "success" for action in execution_result)
        
        if not all_successful:
            return "I couldn't complete that action. Let me know if you'd like to try something else."
        
        # Generate context-aware feedback
        if intent == "pick_up":
            obj = intent_result["entities"].get("objects", ["it"])[0]
            return f"I've picked up the {obj} as requested."
        elif intent == "put_down" and context["active_task"] == "holding_object":
            return "I've put down the object I was holding."
        elif intent == "navigate_to":
            location = intent_result["entities"].get("locations", ["location"])[0]
            return f"I've reached the {location}. What would you like me to do here?"
        else:
            return "I've completed that action. How else can I assist you?"
```

### Confirmation Pattern

For critical actions, use a confirmation step:

```python
class ConfirmationHandler:
    def __init__(self, intent_mapper, action_mapper):
        self.intent_mapper = intent_mapper
        self.action_mapper = action_mapper
        self.speech_synthesizer = None
        self.pending_confirmation = None
    
    def handle_command_with_confirmation(self, voice_command):
        """
        Handle commands with potential confirmation step
        """
        # Classify intent
        intent_result = self.intent_mapper.classify_with_entities(voice_command)
        
        if intent_result["intent"] == "unknown":
            return self.handle_unknown_command(voice_command)
        
        # Determine if action requires confirmation
        if self.requires_confirmation(intent_result):
            confirmation_request = self.generate_confirmation_request(intent_result)
            self.pending_confirmation = {
                "command": voice_command,
                "intent_result": intent_result
            }
            self.provide_feedback(confirmation_request)
            return {
                "status": "awaiting_confirmation",
                "request": confirmation_request
            }
        else:
            # No confirmation needed, proceed directly
            return self.execute_command_without_confirmation(intent_result)
    
    def requires_confirmation(self, intent_result):
        """
        Determine if an action requires confirmation
        """
        high_risk_intents = ["pick_up_valuable", "navigate_to_unmapped_area", "execute_complex_sequence"]
        
        # For now, require confirmation for pick_up actions
        return intent_result["intent"] == "pick_up"
    
    def generate_confirmation_request(self, intent_result):
        """
        Generate a request for user confirmation
        """
        entities = intent_result["entities"]
        obj = entities.get("objects", ["object"])[0] if "objects" in entities else "object"
        
        return f"You asked me to pick up the {obj}. Is that correct? Please say 'yes' to confirm."
    
    def handle_confirmation_response(self, voice_command):
        """
        Handle user's response to confirmation request
        """
        if not self.pending_confirmation:
            return {"status": "no_pending_confirmation"}
        
        confirmation_intent = self.intent_mapper.classify_intent(voice_command.lower().strip())
        
        if confirmation_intent["intent"] in ["affirmative", "yes", "confirm"] or "yes" in voice_command.lower():
            # Execute the pending action
            result = self.execute_command_without_confirmation(
                self.pending_confirmation["intent_result"]
            )
            self.pending_confirmation = None
            return result
        elif confirmation_intent["intent"] in ["negative", "no", "deny"] or "no" in voice_command.lower():
            # Cancel the action
            self.pending_confirmation = None
            self.provide_feedback("Action cancelled as requested.")
            return {"status": "cancelled"}
        else:
            # Unclear response, ask again
            self.provide_feedback("I didn't understand. Please say 'yes' to confirm or 'no' to cancel.")
            return {"status": "unclear_response"}
```

## Voice Interface Components

### Voice Activity Detection

Implement voice activity detection to trigger listening:

```python
import pyaudio
import numpy as np
from scipy.signal import find_peaks

class VoiceActivityDetector:
    def __init__(self, threshold=0.01, silence_duration=1.0):
        self.threshold = threshold  # Amplitude threshold for detecting speech
        self.silence_duration = silence_duration  # Seconds of silence before ending
        self.audio = pyaudio.PyAudio()
        
    def detect_voice_activity(self, duration=5):
        """
        Detect voice activity and return audio data when speech is detected
        """
        # Configuration
        chunk = 1024
        format = pyaudio.paInt16
        channels = 1
        rate = 44100
        
        # Open stream
        stream = self.audio.open(format=format,
                                channels=channels,
                                rate=rate,
                                input=True,
                                frames_per_buffer=chunk)
        
        print("Listening for voice activity...")
        
        # Listen for voice activity
        audio_data = []
        voice_detected = False
        silence_frames = 0
        max_silence_frames = int(self.silence_duration * rate / chunk)
        
        while True:
            data = stream.read(chunk)
            audio_array = np.frombuffer(data, dtype=np.int16) / 32768.0  # Normalize
            
            # Calculate amplitude
            amplitude = np.sqrt(np.mean(audio_array**2))
            
            if amplitude > self.threshold:
                # Voice detected
                voice_detected = True
                audio_data.extend(list(data))
                silence_frames = 0  # Reset silence counter
            elif voice_detected:
                # We were in voice detection mode, now in silence
                silence_frames += 1
                audio_data.extend(list(data))  # Still collecting audio
                
                # Check if we've had enough silence to end
                if silence_frames >= max_silence_frames:
                    break
            # If we haven't detected voice yet, keep listening
            else:
                continue
        
        stream.stop_stream()
        stream.close()
        
        if audio_data:
            # Save the collected audio
            import wave
            filename = "detected_voice.wav"
            wf = wave.open(filename, 'wb')
            wf.setnchannels(channels)
            wf.setsampwidth(self.audio.get_sample_size(format))
            wf.setframerate(rate)
            wf.writeframes(b''.join(audio_data))
            wf.close()
            
            return filename
        else:
            return None
```

### Wake Word Detection

Implement wake word detection to activate the robot:

```python
import speech_recognition as sr
import threading

class WakeWordDetector:
    def __init__(self, wake_words=["robot", "hey robot", "attention"], callback=None):
        self.wake_words = [word.lower() for word in wake_words]
        self.callback = callback
        self.listening = False
        self.recognizer = sr.Recognizer()
        self.microphone = sr.Microphone()
        
        # Adjust for ambient noise
        with self.microphone as source:
            self.recognizer.adjust_for_ambient_noise(source)
    
    def start_listening(self):
        """
        Start listening for wake words
        """
        self.listening = True
        self.listen_thread = threading.Thread(target=self._listen_for_wake_word)
        self.listen_thread.start()
    
    def stop_listening(self):
        """
        Stop listening for wake words
        """
        self.listening = False
    
    def _listen_for_wake_word(self):
        """
        Internal method to listen continuously for wake words
        """
        while self.listening:
            try:
                with self.microphone as source:
                    # Listen for audio with timeout
                    audio = self.recognizer.listen(source, timeout=1, phrase_time_limit=2)
                
                # Recognize speech
                text = self.recognizer.recognize_google(audio).lower()
                
                # Check if any wake word is in the recognized text
                for wake_word in self.wake_words:
                    if wake_word in text:
                        print(f"Wake word detected: {wake_word}")
                        
                        if self.callback:
                            # Call the callback function, passing the recognized text
                            # This allows the callback to start the voice command handler
                            self.callback(text.replace(wake_word, "").strip())
                        
                        break  # Found a wake word, no need to check others
                        
            except sr.WaitTimeoutError:
                # No speech detected, continue listening
                continue
            except sr.UnknownValueError:
                # Recognition failed, continue listening
                continue
            except sr.RequestError as e:
                print(f"Error with speech recognition service: {e}")
                continue

# Example usage
def on_wake_word_detected(remaining_text):
    """
    Callback function when wake word is detected
    """
    print(f"Robot activated. Remaining text: '{remaining_text}'")
    
    # Here you could activate the voice command recognizer
    # and process any additional command that was spoken
    # after the wake word

# Initialize and start wake word detector
# detector = WakeWordDetector(callback=on_wake_word_detected)
# detector.start_listening()
```

## Feedback Systems

### Auditory Feedback

Provide auditory feedback to acknowledge commands:

```python
import pyttsx3
import asyncio
import aiofiles

class AuditoryFeedbackSystem:
    def __init__(self):
        self.tts_engine = pyttsx3.init()
        
        # Configure TTS properties
        voices = self.tts_engine.getProperty('voices')
        if voices:
            self.tts_engine.setProperty('voice', voices[0].id)
        
        self.tts_engine.setProperty('rate', 150)  # Speed of speech
        self.tts_engine.setProperty('volume', 0.8)  # Volume level (0.0 to 1.0)
    
    def speak(self, text):
        """
        Speak the provided text aloud
        """
        print(f"Robot speaking: {text}")
        self.tts_engine.say(text)
        self.tts_engine.runAndWait()
    
    def speak_async(self, text):
        """
        Speak text asynchronously without blocking
        """
        def speak_thread():
            self.tts_engine.say(text)
            self.tts_engine.runAndWait()
        
        import threading
        thread = threading.Thread(target=speak_thread)
        thread.start()
        return thread
    
    def generate_acknowledgment(self, confidence):
        """
        Generate appropriate acknowledgment based on confidence
        """
        if confidence > 0.9:
            return "Got it, executing now."
        elif confidence > 0.7:
            return "I think I heard you. Let me try that."
        elif confidence > 0.5:
            return "I heard something but wasn't sure. Could you repeat that?"
        else:
            return "Sorry, I didn't quite catch that."
```

### Visual Feedback

Use visual indicators to show robot's state:

```python
class VisualFeedbackSystem:
    def __init__(self, robot_display=None):
        self.robot_display = robot_display
        self.current_state = "idle"
        self.feedback_history = []
    
    def update_status(self, status, details=None):
        """
        Update the robot's visual status indicator
        """
        import time
        
        self.current_state = status
        timestamp = time.time()
        
        # Add to history for debugging
        self.feedback_history.append({
            "timestamp": timestamp,
            "status": status,
            "details": details
        })
        
        # Display on robot
        if self.robot_display:
            self.display_status(status, details)
        else:
            # For simulation purposes, print status
            print(f"Robot display: {status} - {details or ''}")
    
    def display_status(self, status, details):
        """
        Display status on robot's screen/LEDs
        """
        # This would control actual hardware display
        # For simulation, we'll just print
        status_icons = {
            "listening": "ðŸ‘‚",
            "processing": "ðŸ§ ", 
            "executing": "ðŸ¤–",
            "completed": "âœ…",
            "error": "âŒ",
            "idle": "â­•"
        }
        
        icon = status_icons.get(status, "?")
        print(f"{icon} {status}: {details}")
    
    def show_attention(self):
        """
        Show that robot is paying attention
        """
        self.update_status("attentive", "Ready for your command")
        # This might involve flashing LEDs or showing animated eyes
    
    def show_processing(self):
        """
        Show that robot is processing the command
        """
        self.update_status("processing", "Understanding your request")
    
    def show_executing(self, action):
        """
        Show action being executed
        """
        self.update_status("executing", f"Doing: {action}")
```

## Integration Example

### Complete Voice-Robot Interaction System

Combine all components into a complete system:

```python
class VoiceRobotInteractionSystem:
    def __init__(self, intent_mapper, action_mapper):
        self.intent_mapper = intent_mapper
        self.action_mapper = action_mapper
        
        # Initialize interaction components
        self.direct_handler = DirectCommandHandler(intent_mapper, action_mapper)
        self.contextual_handler = ContextualConversationHandler(intent_mapper, action_mapper)
        self.confirmation_handler = ConfirmationHandler(intent_mapper, action_mapper)
        self.vad = VoiceActivityDetector()
        self.wake_detector = WakeWordDetector(callback=self.on_wake_word)
        self.auditory_feedback = AuditoryFeedbackSystem()
        self.visual_feedback = VisualFeedbackSystem()
        
        # System state
        self.active_handler = self.direct_handler  # Start with direct command handler
        self.awaiting_confirmation = False
        self.listening_for_command = False
    
    def on_wake_word(self, remaining_text):
        """
        Handle wake word detection
        """
        self.visual_feedback.show_attention()
        self.auditory_feedback.speak("Yes, how can I help you?")
        
        if remaining_text.strip():
            # Process any additional text spoken after wake word
            self.process_command(remaining_text)
    
    def start_interaction(self):
        """
        Start the voice interaction system
        """
        print("Starting voice-robot interaction system...")
        
        # Start wake word detection
        self.wake_detector.start_listening()
        
        # For demonstration, also allow direct commands via keyboard
        print("System active. Say 'robot' to activate or type commands below:")
        
        try:
            while True:
                if not self.awaiting_confirmation:
                    # For demo purposes, also accept keyboard input
                    import sys, select
                    ready, _, _ = select.select([sys.stdin], [], [], 0.1)  # Non-blocking input check
                    if ready:
                        command = sys.stdin.readline().strip()
                        if command.lower() == 'quit':
                            break
                        elif command:
                            self.process_command(command)
        
        except KeyboardInterrupt:
            print("\nShutting down voice-robot interaction system...")
        
        finally:
            self.wake_detector.stop_listening()
    
    def process_command(self, voice_command):
        """
        Process a voice command using the appropriate handler
        """
        if self.awaiting_confirmation:
            # If awaiting confirmation, handle confirmation response
            result = self.confirmation_handler.handle_confirmation_response(voice_command)
            if result["status"] in ["cancelled", "unclear_response"]:
                self.awaiting_confirmation = False
            return result
        
        # Determine which handler to use
        if self.active_handler == self.confirmation_handler:
            # Handle special confirmation cases
            result = self.confirmation_handler.handle_command_with_confirmation(voice_command)
            if result["status"] == "awaiting_confirmation":
                self.awaiting_confirmation = True
            return result
        elif self.active_handler == self.contextual_handler:
            # Use contextual handler
            return self.contextual_handler.handle_conversation_turn(voice_command)
        else:
            # Use direct handler (default)
            return self.direct_handler.handle_direct_command(voice_command)
    
    def switch_handler(self, handler_name):
        """
        Switch between different interaction handlers
        """
        if handler_name == "direct":
            self.active_handler = self.direct_handler
        elif handler_name == "contextual":
            self.active_handler = self.contextual_handler
        elif handler_name == "confirmation":
            self.active_handler = self.confirmation_handler
    
    def get_system_status(self):
        """
        Get current system status
        """
        return {
            "active_handler": type(self.active_handler).__name__,
            "awaiting_confirmation": self.awaiting_confirmation,
            "wake_detection_active": self.wake_detector.listening,
            "conversation_context": getattr(self.contextual_handler, "conversation_context", {})
        }
```

## Testing and Validation

### Interaction Pattern Testing

Create tests for the voice-robot interaction patterns:

```python
class VoiceInteractionTester:
    def __init__(self, interaction_system):
        self.system = interaction_system
        self.test_results = []
    
    def test_direct_command_pattern(self):
        """
        Test the direct command pattern
        """
        test_commands = [
            {"input": "Move forward 1 meter", "expected_intent": "move_forward"},
            {"input": "Turn left", "expected_intent": "turn_left"},
            {"input": "Pick up the red ball", "expected_intent": "pick_up"}
        ]
        
        results = []
        for test in test_commands:
            # Simulate command processing
            result = self.system.direct_handler.handle_direct_command(test["input"])
            
            success = result.get("intent") == test["expected_intent"]
            results.append({
                "input": test["input"],
                "expected": test["expected_intent"],
                "actual": result.get("intent"),
                "success": success,
                "feedback": result.get("feedback", "")
            })
        
        self.test_results.append({
            "test_type": "direct_command_pattern",
            "results": results,
            "passed": sum(1 for r in results if r["success"]),
            "total": len(results)
        })
        
        return results
    
    def test_contextual_pattern(self):
        """
        Test the contextual conversation pattern
        """
        # Simulate a conversation
        conversation = [
            ("Navigate to the table", "navigate_to"),
            ("Pick up the blue cup", "pick_up"),
            ("Put it down", "put_down")
        ]
        
        results = []
        for command, expected_intent in conversation:
            result = self.system.contextual_handler.handle_conversation_turn(command)
            
            success = result.get("intent") == expected_intent
            results.append({
                "input": command,
                "expected": expected_intent,
                "actual": result.get("intent"),
                "success": success,
                "context": result.get("context", {})
            })
        
        self.test_results.append({
            "test_type": "contextual_pattern",
            "results": results,
            "passed": sum(1 for r in results if r["success"]),
            "total": len(results)
        })
        
        return results
    
    def generate_comprehensive_report(self):
        """
        Generate a comprehensive test report
        """
        if not self.test_results:
            return "No tests have been run yet."
        
        total_passed = 0
        total_tests = 0
        
        report = "Voice-Robot Interaction System Test Report\n"
        report += "=" * 50 + "\n\n"
        
        for test_group in self.test_results:
            passed = test_group["passed"]
            total = test_group["total"]
            success_rate = passed / total if total > 0 else 0
            
            report += f"{test_group['test_type'].replace('_', ' ').title()}:\n"
            report += f"  Tests: {passed}/{total} passed ({success_rate:.1%})\n"
            
            for result in test_group["results"]:
                status = "âœ“ PASS" if result["success"] else "âœ— FAIL" 
                report += f"  {status}: '{result['input']}' -> {result['actual']} (expected {result['expected']})\n"
            
            report += "\n"
            
            total_passed += passed
            total_tests += total
        
        overall_success_rate = total_passed / total_tests if total_tests > 0 else 0
        report += f"Overall: {total_passed}/{total_tests} tests passed ({overall_success_rate:.1%})"
        
        return report
```

## Troubleshooting Common Issues

### Common Voice Interaction Problems

1. **Recognition Errors**: Implement fallback strategies
2. **Context Loss**: Maintain and restore context appropriately
3. **Response Timing**: Optimize processing for natural-feeling responses
4. **Ambiguous Commands**: Improve disambiguation strategies

### Handling Noisy Environments

```python
def handle_noisy_environment(original_command, audio_quality):
    """
    Handle voice commands in noisy environments
    """
    if audio_quality < 0.3:  # Poor audio quality
        return {
            "status": "request_repeat",
            "message": "I had trouble hearing you clearly. Could you please repeat your command?"
        }
    elif audio_quality < 0.6:  # Moderate noise
        return {
            "status": "proceed_with_caution",
            "confidence_adjustment": -0.1  # Lower confidence threshold
        }
    else:  # Good audio quality
        return {
            "status": "normal_processing"
        }
```

## Summary

Voice-robot interaction design is a critical aspect of creating intuitive humanoid robots. This chapter covered:

1. **Interaction Patterns**: Direct commands, contextual conversations, and confirmation workflows
2. **Core Components**: Voice activity detection, wake word recognition, and feedback systems
3. **Integration**: Combining all components into a cohesive system
4. **Testing**: Validating interaction patterns and system behavior

The key to effective voice-robot interaction lies in making the robot feel responsive, reliable, and natural to interact with. Well-designed feedback systems and appropriate handling of ambiguous or unclear commands contribute significantly to a positive user experience.

In the next chapter, we'll explore how to implement the complete voice-to-action pipeline that connects all these components together.